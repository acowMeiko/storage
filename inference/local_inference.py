"""
æœ¬åœ°æ¨¡å‹æ¨ç†æ¨¡å—
æ”¯æŒvLLMæ‰¹é‡æ¨ç†å’Œå•æ¡æ¨ç†
"""
import logging
from typing import List, Optional, Union
from dataclasses import dataclass
import config
from template.prompt_template import TASK_DESC_PROMPT, DIRECT_ANSWER_PROMPT, GUIDED_ANSWER_PROMPT,format
logger = logging.getLogger(__name__)

# å…¨å±€æ¨¡å‹å®ä¾‹
_vllm_model = None
_sampling_params = None


@dataclass
class InferenceConfig:
    """æ¨ç†é…ç½®"""
    temperature: float = config.DEFAULT_TEMPERATURE
    top_p: float = config.DEFAULT_TOP_P
    max_tokens: int = config.DEFAULT_MAX_TOKENS
    stop: Optional[List[str]] = None


def get_vllm_model():
    """
    è·å–æˆ–åˆå§‹åŒ–vLLMæ¨¡å‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    
    Returns:
        vLLMæ¨¡å‹å®ä¾‹
    """
    global _vllm_model, _sampling_params
    
    if _vllm_model is None:
        try:
            import os
            from vllm import LLM, SamplingParams
            
            # å¦‚æœç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œåˆ™é»˜è®¤ä½¿ç”¨ 0,1
            if 'CUDA_VISIBLE_DEVICES' not in os.environ:
                os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
                logger.info(f"ç¯å¢ƒå˜é‡ CUDA_VISIBLE_DEVICES æœªè®¾ç½®ï¼Œé»˜è®¤ä½¿ç”¨: {os.environ['CUDA_VISIBLE_DEVICES']}")
            else:
                logger.info(f"ä½¿ç”¨ç¯å¢ƒå˜é‡ CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}")
            
            logger.info("=" * 60)
            logger.info("åˆå§‹åŒ–vLLMæœ¬åœ°æ¨¡å‹...")
            logger.info(f"æ¨¡å‹è·¯å¾„: {config.BASE_MODEL_NAME}")
            logger.info("=" * 60)
            
            _vllm_model = LLM(
                model=config.lora_model_path,
                tensor_parallel_size=2,  # 2å¼ GPUå¹¶è¡Œ
                gpu_memory_utilization=0.9,  # 80GBæ˜¾å­˜ï¼Œå¯ä»¥ä½¿ç”¨æ›´é«˜åˆ©ç”¨ç‡
                trust_remote_code=True,  # ä¿¡ä»»è¿œç¨‹ä»£ç ï¼ˆæŸäº›æ¨¡å‹éœ€è¦ï¼‰
                dtype="auto",  # è‡ªåŠ¨é€‰æ‹©æ•°æ®ç±»å‹
                max_model_len=config.MAX_MODEL_LEN,  # ä½¿ç”¨é…ç½®çš„æœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤8192ï¼‰
            )
            
            _sampling_params = SamplingParams(
                temperature=config.DEFAULT_TEMPERATURE,
                top_p=config.DEFAULT_TOP_P,
                max_tokens=2048,  # ä½¿ç”¨é…ç½®çš„æœ€å¤§tokenæ•°
            )
            
            logger.info("vLLMæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            
        except ImportError:
            logger.error("æœªå®‰è£…vLLMï¼Œè¯·è¿è¡Œ: pip install vllm")
            raise
        except Exception as e:
            logger.error(f"vLLMæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            raise
    
    return _vllm_model, _sampling_params


def batch_inference(
    prompts: List[str],
    temperature: Optional[float] = None,
    top_p: Optional[float] = None,
    max_tokens: Optional[int] = None,
    use_tqdm: bool = True
) -> List[str]:
    """
    æ‰¹é‡æ¨ç†
    
    Args:
        prompts: æç¤ºè¯åˆ—è¡¨
        temperature: æ¸©åº¦å‚æ•°ï¼ˆå¯é€‰ï¼‰
        top_p: top_på‚æ•°ï¼ˆå¯é€‰ï¼‰
        max_tokens: æœ€å¤§tokenæ•°ï¼ˆå¯é€‰ï¼‰
        use_tqdm: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        
    Returns:
        ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
    """
    if not prompts:
        return []
    
    model, default_params = get_vllm_model()
    
    # åˆ›å»ºé‡‡æ ·å‚æ•°
    from vllm import SamplingParams
    sampling_params = SamplingParams(
        temperature=temperature or config.DEFAULT_TEMPERATURE,
        top_p=top_p or config.DEFAULT_TOP_P,
        max_tokens=max_tokens or config.DEFAULT_MAX_TOKENS,
    )
    
    logger.debug(f"æ‰¹é‡æ¨ç†: {len(prompts)} æ¡æ•°æ®")
    
    # æ£€æµ‹promptæ˜¯å¦å·²ç»æ ¼å¼åŒ–ï¼ˆé¿å…åŒé‡æ ¼å¼åŒ–ï¼‰
    # å¦‚æœpromptä¸­å·²åŒ…å«<|im_start|>æ ‡è®°ï¼Œè¯´æ˜å·²ç»æ ¼å¼å
    try:
        # vLLMæ‰¹é‡ç”Ÿæˆ - ç¦ç”¨å†…éƒ¨è¿›åº¦æ¡é¿å…è¾“å‡ºæ··ä¹±
        outputs = model.generate(prompts, sampling_params, use_tqdm=False)
        
        # æå–ç”Ÿæˆçš„æ–‡æœ¬
        results = [output.outputs[0].text.strip() for output in outputs]
        
        return results
        
    except Exception as e:
        logger.error(f"æ‰¹é‡æ¨ç†å¤±è´¥: {e}", exc_info=True)
        raise


def single_inference(
    prompt: str,
    temperature: Optional[float] = None,
    top_p: Optional[float] = None,
    max_tokens: Optional[int] = None
) -> str:
    """
    å•æ¡æ¨ç†
    
    Args:
        prompt: æç¤ºè¯
        temperature: æ¸©åº¦å‚æ•°ï¼ˆå¯é€‰ï¼‰
        top_p: top_på‚æ•°ï¼ˆå¯é€‰ï¼‰
        max_tokens: æœ€å¤§tokenæ•°ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        ç”Ÿæˆçš„æ–‡æœ¬
    """
    results = batch_inference(
        [prompt],
        temperature=temperature,
        top_p=top_p,
        max_tokens=max_tokens,
        use_tqdm=False
    )
    return results[0] if results else ""


def cleanup_model():
    """æ¸…ç†æ¨¡å‹ï¼Œé‡Šæ”¾æ˜¾å­˜"""
    global _vllm_model, _sampling_params
    
    if _vllm_model is not None:
        logger.info("æ¸…ç†vLLMæ¨¡å‹...")
        del _vllm_model
        _vllm_model = None
        _sampling_params = None
        
        # æ¸…ç†CUDAç¼“å­˜
        try:
            import torch
            torch.cuda.empty_cache()
            logger.info("CUDAç¼“å­˜å·²æ¸…ç†")
        except:
            pass


# æ³¨å†Œé€€å‡ºæ—¶æ¸…ç†
import atexit
atexit.register(cleanup_model)

